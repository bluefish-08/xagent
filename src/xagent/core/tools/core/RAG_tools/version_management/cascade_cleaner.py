"""Cascade cleanup functions for version management.

Provide cascade cleanup utilities when promoting main versions,
ensuring data consistency across processing stages.
"""

from __future__ import annotations

import logging
from typing import Any, Dict, Optional

from ......providers.vector_store.lancedb import get_connection_from_env
from ..core.exceptions import CascadeCleanupError
from ..LanceDB.schema_manager import (
    ensure_chunks_table,
    ensure_documents_table,
    ensure_main_pointers_table,
    ensure_parses_table,
)
from ..utils.string_utils import build_lancedb_filter_expression, escape_lancedb_string
from .main_pointer_manager import get_main_pointer

logger = logging.getLogger(__name__)


def _plan_by_predicates(
    conn: Any, table_to_filter: Dict[str, str], model_tag: Optional[str] = None
) -> Dict[str, int]:
    """Count rows that match each table predicate without deleting.

    Args:
        conn: LanceDB connection
        table_to_filter: Mapping of table name -> filter expression
        model_tag: Optional model tag to filter embeddings tables. If specified,
                   only the embeddings table matching this model will be counted.

    Returns:
        Mapping of table name -> matched row count
    """
    counts: Dict[str, int] = {}
    table_names = conn.table_names()
    for table_name, filt in table_to_filter.items():
        # Special fan-out handling for embeddings preview like deleter
        if table_name == "__embeddings__":
            total = 0
            all_embed_tables = [t for t in table_names if t.startswith("embeddings_")]
            # Apply model_tag filter if specified (must match _delete_by_predicates logic)
            if model_tag:
                all_embed_tables = [
                    t for t in all_embed_tables if t == f"embeddings_{model_tag}"
                ]
            for t in all_embed_tables:
                table = conn.open_table(t)
                count = table.count_rows(filt)
                total += count
            counts[table_name] = total
            continue

        if table_name not in table_names:
            counts[table_name] = 0
            continue
        table = conn.open_table(table_name)
        count = table.count_rows(filt)
        counts[table_name] = count
    return counts


def _delete_by_predicates(
    conn: Any, table_to_filter: Dict[str, str], model_tag: Optional[str] = None
) -> Dict[str, int]:
    """Delete rows by table predicates in a fixed, safe order.

    Order: embeddings_* -> chunks -> parses -> main_pointers -> documents
    Unknown tables are executed after the known order, in given insertion order.

    Args:
        conn: LanceDB connection
        table_to_filter: Dictionary mapping table names to filter expressions
        model_tag: Optional model tag to filter embeddings tables. If specified,
                   only the embeddings table matching this model will be processed.
    """
    deleted: Dict[str, int] = {}
    table_names = conn.table_names()

    order = [
        # embeddings handled specially below (fan-out across many tables)
        "__embeddings__",
        "chunks",
        "parses",
        "main_pointers",
        "documents",
    ]

    # First handle embeddings fan-out
    if "__embeddings__" in table_to_filter:
        filt = table_to_filter["__embeddings__"]
        total = 0

        # Filter embeddings tables based on model_tag if specified
        all_embed_tables = [t for t in table_names if t.startswith("embeddings_")]
        if model_tag is not None:
            target_tables = [
                t for t in all_embed_tables if t == f"embeddings_{model_tag}"
            ]
        else:
            target_tables = all_embed_tables

        for t in target_tables:
            table = conn.open_table(t)
            cnt = table.count_rows(filt)
            if cnt > 0:
                table.delete(filt)
            total += cnt
        deleted["embeddings"] = total
        if total > 0:
            logger.info(f"Cascade cleanup: deleted {total} rows from embeddings tables")

    # Then handle known tables
    for name in order[1:]:
        if name in table_to_filter and name in table_names:
            filt = table_to_filter[name]
            table = conn.open_table(name)
            cnt = table.count_rows(filt)
            if cnt > 0:
                table.delete(filt)
                logger.info(f"Cascade cleanup: deleted {cnt} rows from {name}")
            deleted[name] = cnt

    # Finally, handle any remaining custom tables once
    for name, filt in table_to_filter.items():
        if name in ("__embeddings__", "chunks", "parses", "main_pointers", "documents"):
            continue
        if name not in table_names:
            deleted[name] = 0
            continue
        table = conn.open_table(name)
        cnt = table.count_rows(filt)
        if cnt > 0:
            table.delete(filt)
            logger.info(f"Cascade cleanup: deleted {cnt} rows from {name}")
        deleted[name] = cnt

    return deleted


def cleanup_cascade(
    collection: str,
    doc_id: str,
    scope: str,
    new_parse_hash: Optional[str] = None,
    old_parse_hash: Optional[str] = None,
    model_tag: Optional[str] = None,
    preview_only: bool = True,
    confirm: bool = False,
) -> Dict[str, int]:
    """Unified cascade cleanup by scope with preview/confirm semantics.

    Args:
        collection: Collection name
        doc_id: Document ID
        scope: "document" | "parse" | "chunk" | "embeddings" | "pointers"
        new_parse_hash: New main parse hash for parse/chunk scopes
        old_parse_hash: Optional old main parse hash (auto-filled from pointers if None)
        model_tag: Optional embed model tag limiter
        preview_only: If True, only plan counts
        confirm: If True, execute deletions

    Returns:
        Deleted (or planned) counts per table scope
    """
    conn = get_connection_from_env()
    ensure_documents_table(conn)
    ensure_parses_table(conn)
    ensure_chunks_table(conn)
    ensure_main_pointers_table(conn)

    predicates: Dict[str, str] = {}

    if scope == "document":
        filt = build_lancedb_filter_expression(
            {"collection": collection, "doc_id": doc_id}
        )
        predicates = {
            "__embeddings__": filt,
            "chunks": filt,
            "parses": filt,
            "main_pointers": filt,
            "documents": filt,
        }
    elif scope == "parse":
        # Fill old from pointer if needed
        if old_parse_hash is None:
            pointer = get_main_pointer(collection, doc_id, "parse")
            old_parse_hash = pointer["technical_id"] if pointer else None

        if old_parse_hash:
            # Build safe filter expression for old parse hash
            base_filters = {
                "collection": collection,
                "doc_id": doc_id,
                "parse_hash": old_parse_hash,
            }
            base = build_lancedb_filter_expression(base_filters)
            predicates["__embeddings__"] = base
            predicates["chunks"] = base
        if new_parse_hash:
            # Build safe filter expression for new parse hash (using != operator)
            escaped_collection = escape_lancedb_string(collection)
            escaped_doc_id = escape_lancedb_string(doc_id)
            escaped_new_parse_hash = escape_lancedb_string(new_parse_hash)
            other = f"collection == '{escaped_collection}' AND doc_id == '{escaped_doc_id}' AND parse_hash != '{escaped_new_parse_hash}'"
            predicates["__embeddings__"] = other
            predicates["chunks"] = other
            predicates["parses"] = other
    elif scope == "chunk":
        if old_parse_hash is None:
            pointer = get_main_pointer(collection, doc_id, "chunk")
            old_parse_hash = pointer["technical_id"] if pointer else None
        if old_parse_hash:
            # Build safe filter expression for old parse hash
            base_filters = {
                "collection": collection,
                "doc_id": doc_id,
                "parse_hash": old_parse_hash,
            }
            base = build_lancedb_filter_expression(base_filters)
            predicates["__embeddings__"] = base
        if new_parse_hash:
            # Note: For != operator, we need to manually construct the filter
            # as build_lancedb_filter_expression only supports == operator
            escaped_collection = escape_lancedb_string(collection)
            escaped_doc_id = escape_lancedb_string(doc_id)
            escaped_parse_hash = escape_lancedb_string(new_parse_hash)
            other = f"collection == '{escaped_collection}' AND doc_id == '{escaped_doc_id}' AND parse_hash != '{escaped_parse_hash}'"
            predicates["__embeddings__"] = other
            predicates["chunks"] = other
    elif scope == "embeddings":
        # Currently generic deleter applies to all embeddings tables
        filt = build_lancedb_filter_expression(
            {"collection": collection, "doc_id": doc_id}
        )
        predicates["__embeddings__"] = filt
    elif scope == "pointers":
        filt = build_lancedb_filter_expression(
            {"collection": collection, "doc_id": doc_id}
        )
        predicates["main_pointers"] = filt
    else:
        raise CascadeCleanupError(f"Unsupported scope: {scope}")

    if preview_only and not confirm:
        planned = _plan_by_predicates(conn, predicates, model_tag=model_tag)
        # normalize internal fan-out key for embeddings
        if "__embeddings__" in planned:
            planned["embeddings"] = planned.pop("__embeddings__")
        return planned

    return _delete_by_predicates(conn, predicates, model_tag=model_tag)


def cleanup_document_cascade(
    collection: str,
    doc_id: str,
    model_tag: Optional[str] = None,
    preview_only: bool = True,
    confirm: bool = False,
) -> Dict[str, int]:
    """Cascade delete all data for a document across all stages.

    Order: embeddings_* -> chunks -> parses -> main_pointers -> documents

    Args:
        collection: Collection name
        doc_id: Document ID
        model_tag: Optional model tag to limit embeddings deletion

    Returns:
        Deleted counts per scope
    """
    try:
        # Delegate to unified entry
        return cleanup_cascade(
            collection=collection,
            doc_id=doc_id,
            scope="document",
            model_tag=model_tag,
            preview_only=preview_only,
            confirm=confirm,
        )

    except Exception as e:
        raise CascadeCleanupError(f"Failed to cleanup document cascade: {e}")


def cleanup_parse_cascade(
    collection: str,
    doc_id: str,
    old_parse_hash: Optional[str] = None,
    new_parse_hash: Optional[str] = None,
    preview_only: bool = True,
    confirm: bool = False,
) -> Dict[str, int]:
    """Clean up cascade when promoting a new parse version.

    This method:
    1. Deletes old parse's chunks and embeddings
    2. Deletes other parse candidates and their downstream data

    Args:
        collection: Collection name
        doc_id: Document ID
        old_parse_hash: Old main parse hash (optional)
        new_parse_hash: New main parse hash (optional)

    Returns:
        Dictionary with deletion counts

    Raises:
        CascadeCleanupError: If cleanup fails
    """
    try:
        return cleanup_cascade(
            collection=collection,
            doc_id=doc_id,
            scope="parse",
            new_parse_hash=new_parse_hash,
            old_parse_hash=old_parse_hash,
            preview_only=preview_only,
            confirm=confirm,
        )

    except Exception as e:
        raise CascadeCleanupError(f"Failed to cleanup parse cascade: {e}")


def cleanup_chunk_cascade(
    collection: str,
    doc_id: str,
    old_parse_hash: Optional[str] = None,
    new_parse_hash: Optional[str] = None,
    preview_only: bool = True,
    confirm: bool = False,
) -> Dict[str, int]:
    """Clean up cascade when promoting a new chunk version.

    This method:
    1. Deletes old chunk's embeddings
    2. Deletes other chunk candidates

    Args:
        collection: Collection name
        doc_id: Document ID
        old_parse_hash: Old main parse hash (optional)
        new_parse_hash: New main parse hash (optional)

    Returns:
        Dictionary with deletion counts

    Raises:
        CascadeCleanupError: If cleanup fails
    """
    try:
        return cleanup_cascade(
            collection=collection,
            doc_id=doc_id,
            scope="chunk",
            new_parse_hash=new_parse_hash,
            old_parse_hash=old_parse_hash,
            preview_only=preview_only,
            confirm=confirm,
        )

    except Exception as e:
        raise CascadeCleanupError(f"Failed to cleanup chunk cascade: {e}")


def cleanup_embed_cascade(
    collection: str,
    doc_id: str,
    model_tag: Optional[str] = None,
    old_technical_id: Optional[str] = None,
    new_technical_id: Optional[str] = None,
    preview_only: bool = True,
    confirm: bool = False,
) -> Dict[str, int]:
    """Clean up cascade when promoting a new embeddings version.

    This method:
    1. Deletes other embeddings candidates (optionally filtered by model_tag)

    Args:
        collection: Collection name
        doc_id: Document ID
        model_tag: Model tag filter (optional)
        old_technical_id: Old main technical ID (optional)
        new_technical_id: New main technical ID (optional)

    Returns:
        Dictionary with deletion counts

    Raises:
        CascadeCleanupError: If cleanup fails
    """
    try:
        # Delegate to unified entry; old/new technical ids are not used in current schema
        return cleanup_cascade(
            collection=collection,
            doc_id=doc_id,
            scope="embeddings",
            model_tag=model_tag,
            preview_only=preview_only,
            confirm=confirm,
        )

    except Exception as e:
        raise CascadeCleanupError(f"Failed to cleanup embed cascade: {e}")
